{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InterFaceGAN",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamelof23/ASL/blob/main/InterFaceGAN-Jam1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJDJLE3v0HNr"
      },
      "source": [
        "# Fetch Codebase and Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqiWKjpFa0ov"
      },
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'interfacegan'\n",
        "!git clone https://github.com/genforce/interfacegan.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "!wget https://www.dropbox.com/s/t74z87pk3cf8ny7/pggan_celebahq.pth?dl=1 -O models/pretrain/pggan_celebahq.pth --quiet\n",
        "!wget https://www.dropbox.com/s/nmo2g3u0qt7x70m/stylegan_celebahq.pth?dl=1 -O models/pretrain/stylegan_celebahq.pth --quiet\n",
        "!wget https://www.dropbox.com/s/qyv37eaobnow7fu/stylegan_ffhq.pth?dl=1 -O models/pretrain/stylegan_ffhq.pth --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ_IXBZr8YcJ"
      },
      "source": [
        "# Define Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijKTlG5GeTd3"
      },
      "source": [
        "import os.path\n",
        "import io\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "import cv2\n",
        "import PIL.Image\n",
        "\n",
        "import torch\n",
        "\n",
        "# A dictionary containing information about the models available (e.g., pggan, stylegan).\n",
        "from models.model_settings import MODEL_POOL\n",
        "\n",
        "from models.pggan_generator import PGGANGenerator\n",
        "from models.stylegan_generator import StyleGANGenerator\n",
        "from utils.manipulator import linear_interpolate\n",
        "\n",
        "\n",
        "def build_generator(model_name):\n",
        "  \"\"\"Builds the generator by model name.\"\"\"\n",
        "  gan_type = MODEL_POOL[model_name]['gan_type']\n",
        "  if gan_type == 'pggan':\n",
        "    generator = PGGANGenerator(model_name)\n",
        "  elif gan_type == 'stylegan':\n",
        "    generator = StyleGANGenerator(model_name)\n",
        "  return generator\n",
        "\n",
        "\n",
        "def sample_codes(generator, num, latent_space_type='Z', seed=0):\n",
        "  \"\"\"Samples latent codes randomly.\"\"\"\n",
        "  #Setting a random seed ensures reproducibility. Every time the function is called with the same seed, it generates the same latent codes are typically sampled from a standard normal distribution.\n",
        "  np.random.seed(seed)\n",
        "  #The function uses generator.easy_sample(num) to sample num latent vectors from the Z space.\n",
        "  codes = generator.easy_sample(num)\n",
        "  print(f\"[INFO] Latent codes in Z space (before mapping):\")\n",
        "  print(f\"Shape: {codes.shape}\")\n",
        "  print(f\"Sample data: {codes[0, :5]}\")  # Print first 5 elements of the first vector\n",
        "  #This condition checks if the generator is using StyleGAN and whether we want to use the W space.\n",
        "  if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "    #he codes sampled in Z space are first converted to a PyTorch tensor and moved to the device (CPU or GPU):\n",
        "    codes = torch.from_numpy(codes).type(torch.FloatTensor).to(generator.run_device)\n",
        "    # Print information before mapping\n",
        "    print(f\"[INFO] Latent codes as tensor (before mapping to W space):\")\n",
        "    print(f\"Shape: {codes.shape}\")\n",
        "    print(f\"Sample data: {codes[0, :5]}\") #extracts the first 5 elements of the first latent vector.\n",
        "    #The codes are then passed through the StyleGAN's mapping network to convert them into the W space:\n",
        "    codes = generator.get_value(generator.model.mapping(codes))\n",
        "    # Print information after mapping to W space\n",
        "    print(f\"[INFO] Latent codes in W space (after mapping):\")\n",
        "    print(f\"Shape: {codes.shape}\")\n",
        "    print(f\"Sample data: {codes[0, :5]}\") #extracts the first 5 elements of the first latent vector.\n",
        "  #Finally, the function returns the latent codes, which can either be in the Z space or transformed to the W space, depending on the inputs.\n",
        "  #The mapping network transforms Z vectors into the W space, which is disentangled.\n",
        "  return codes\n",
        "\n",
        "\n",
        "#main function display a grid of images\n",
        "\n",
        "#images: A NumPy array containing multiple images. The expected shape is (num, height, width, channels), where:\n",
        "#col: The number of columns to display in the grid.\n",
        "#viz_size: The size (in pixels) to which each image will be resized before displaying (default is 256).\n",
        "def imshow(images, col, viz_size=256):\n",
        "  \"\"\"Shows images in one figure.\"\"\"\n",
        "  # Extracting the Shape of the Images\n",
        "  num, height, width, channels = images.shape\n",
        "  #Ensures that the number of images (num) is divisible by the number of columns (col).\n",
        "  #This makes sure that the images can be arranged into a complete grid without leaving any empty cells.\n",
        "  assert num % col == 0\n",
        "  #Computes the number of rows needed to fit all the images in the grid.\n",
        "  row = num // col\n",
        "\n",
        "  #Creates an empty canvas (a blank image) to hold all the images in a grid format.\n",
        "  #The canvas has dimensions (viz_size * row, viz_size * col, channels), where:\n",
        "  #viz_size * row: Total height of the grid.\n",
        "  #viz_size * col: Total width of the grid.\n",
        "  #channels: Number of color channels (same as the input images).\n",
        "  fused_image = np.zeros((viz_size * row, viz_size * col, channels), dtype=np.uint8)\n",
        "\n",
        "  #Placing Each Image on the Canvas\n",
        "  for idx, image in enumerate(images):\n",
        "    i, j = divmod(idx, col)\n",
        "    y = i * viz_size\n",
        "    x = j * viz_size\n",
        "    #Resizing and Copying Each Image to the Canvas\n",
        "    if height != viz_size or width != viz_size:\n",
        "      image = cv2.resize(image, (viz_size, viz_size))\n",
        "    fused_image[y:y + viz_size, x:x + viz_size] = image\n",
        "\n",
        "  #Converting the Canvas to a Displayable Format\n",
        "  fused_image = np.asarray(fused_image, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(fused_image).save(data, 'jpeg')\n",
        "  im_data = data.getvalue()\n",
        "  #Displaying the Image Using IPython\n",
        "  disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  return disp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7gkmrVW8eR1"
      },
      "source": [
        "# Select a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoWI4fPQ6Gnf"
      },
      "source": [
        "#This line is a special Jupyter Notebook cell annotation, display-mode: \"form\": Displays the code as a collapsible form in the notebook,\n",
        "#run: \"auto\": Automatically runs the cell whenever you change any of the parameters in the form.\n",
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "model_name = \"stylegan_ffhq\" #@param ['pggan_celebahq','stylegan_celebahq', 'stylegan_ffhq']\n",
        "latent_space_type = \"W\" #@param ['Z', 'W']\n",
        "\n",
        "#The function build_generator(model_name) is defined elsewhere and is responsible for loading the appropriate model based on the model_name parameter.\n",
        "generator = build_generator(model_name)\n",
        "\n",
        "# A list of attributes that you want to manipulate in the generated images.\n",
        "ATTRS = ['age', 'eyeglasses', 'gender', 'pose', 'smile']\n",
        "#boundaries: An empty dictionary that will store the attribute boundaries for each of the attributes in ATTRS\n",
        "boundaries = {}\n",
        "\n",
        "#Loading Attribute Boundaries\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  boundary_name = f'{model_name}_{attr_name}'\n",
        "  #np.load() loads the boundary vectors from .npy files, which contain pre-computed directions in the latent space that control the attributes.\n",
        "  if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "    boundaries[attr_name] = np.load(f'boundaries/{boundary_name}_w_boundary.npy')\n",
        "  else:\n",
        "    boundaries[attr_name] = np.load(f'boundaries/{boundary_name}_boundary.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDStH1O5t1KC"
      },
      "source": [
        "# Sample latent codes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlRGKZbJt9hA"
      },
      "source": [
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "num_samples = 4 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "noise_seed = 0 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "\n",
        "#sample_codes(): This function samples latent vectors based on the num_samples and noise_seed parameters.\n",
        "#The latent_space_type parameter specifies whether to sample in the Z space or the W space.\n",
        "#The result, latent_codes, is a batch of latent vectors that will be used to generate images.\n",
        "latent_codes = sample_codes(generator, num_samples, latent_space_type, noise_seed)\n",
        "# checks the type of generator being used.\n",
        "if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "  #If the generator is StyleGAN and the W space is selected, it sets synthesis_kwargs to include 'latent_space_type': 'W'.\n",
        "  synthesis_kwargs = {'latent_space_type': 'W'}\n",
        "else:\n",
        "  #For other cases (like Z space or PGGAN), no additional arguments are needed, so synthesis_kwargs remains an empty dictionary.\n",
        "  synthesis_kwargs = {}\n",
        "\n",
        "#synthesis_kwargs is a dictionary that contains optional arguments for the image synthesis function (easy_synthesize).\n",
        "#If the generator type is 'stylegan' and you have chosen to use the W space, then\n",
        "#synthesis_kwargs is set to {'latent_space_type': 'W'}.\n",
        "#This tells the easy_synthesize method to use the W space for generating images.\n",
        "\n",
        "#The **synthesis_kwargs unpacks the dictionary and passes its contents as keyword arguments({'latent_space_type': 'W'}) to the easy_synthesize() function.\n",
        "# 'latent_space_type': 'W' passed This ensures that the generator uses the W space for synthesis.\n",
        "# use synthesis_kwargs when You are using StyleGAN and want to generate images in the W space, which provides more control and disentangled features.\n",
        "# Exclude when You are using StyleGAN with the Z space.\n",
        "images = generator.easy_synthesize(latent_codes, **synthesis_kwargs)['image']\n",
        "imshow(images, col=num_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmRPN3xz8jCH"
      },
      "source": [
        "# Edit facial attributes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccONBF60mVir"
      },
      "source": [
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "age = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "eyeglasses = 0 #@param {type:\"slider\", min:-2.9, max:3.0, step:0.1}\n",
        "gender = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "pose = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "smile = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "\n",
        "# Copying the Original Latent Codes\n",
        "new_codes = latent_codes.copy()\n",
        "\n",
        "#The loop iterates over the list of attributes (ATTRS), which is defined earlier in  code:\n",
        "# boundaries[attr_name]: pre-computed direction vector in the latent space\n",
        "#eval(attr_name): This converts the name of the attribute (e.g., 'age', 'smile') into its current value from the sliders.\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  new_codes += boundaries[attr_name] * eval(attr_name)\n",
        "\n",
        "new_images = generator.easy_synthesize(new_codes, **synthesis_kwargs)['image']\n",
        "imshow(new_images, col=num_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All"
      ],
      "metadata": {
        "id": "7YPcXjTOCObU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'interfacegan'\n",
        "!git clone https://github.com/genforce/interfacegan.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "!wget https://www.dropbox.com/s/t74z87pk3cf8ny7/pggan_celebahq.pth?dl=1 -O models/pretrain/pggan_celebahq.pth --quiet\n",
        "!wget https://www.dropbox.com/s/nmo2g3u0qt7x70m/stylegan_celebahq.pth?dl=1 -O models/pretrain/stylegan_celebahq.pth --quiet\n",
        "!wget https://www.dropbox.com/s/qyv37eaobnow7fu/stylegan_ffhq.pth?dl=1 -O models/pretrain/stylegan_ffhq.pth --quiet\n",
        "\n",
        "import os.path\n",
        "import io\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "import cv2\n",
        "import PIL.Image\n",
        "\n",
        "import torch\n",
        "\n",
        "# A dictionary containing information about the models available (e.g., pggan, stylegan).\n",
        "from models.model_settings import MODEL_POOL\n",
        "\n",
        "from models.pggan_generator import PGGANGenerator\n",
        "from models.stylegan_generator import StyleGANGenerator\n",
        "from utils.manipulator import linear_interpolate\n",
        "\n",
        "\n",
        "def build_generator(model_name):\n",
        "  \"\"\"Builds the generator by model name.\"\"\"\n",
        "  gan_type = MODEL_POOL[model_name]['gan_type']\n",
        "  if gan_type == 'pggan':\n",
        "    generator = PGGANGenerator(model_name)\n",
        "  elif gan_type == 'stylegan':\n",
        "    generator = StyleGANGenerator(model_name)\n",
        "  return generator\n",
        "\n",
        "\n",
        "def sample_codes(generator, num, latent_space_type='Z', seed=0):\n",
        "  \"\"\"Samples latent codes randomly.\"\"\"\n",
        "  #Setting a random seed ensures reproducibility. Every time the function is called with the same seed, it generates the same latent codes are typically sampled from a standard normal distribution.\n",
        "  np.random.seed(seed)\n",
        "  #The function uses generator.easy_sample(num) to sample num latent vectors from the Z space.\n",
        "  codes = generator.easy_sample(num)\n",
        "  print(f\"[INFO] Latent codes in Z space (before mapping):\")\n",
        "  print(f\"Shape: {codes.shape}\")\n",
        "  print(f\"Sample data: {codes[0, :5]}\")  # Print first 5 elements of the first vector\n",
        "  #This condition checks if the generator is using StyleGAN and whether we want to use the W space.\n",
        "  if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "    #he codes sampled in Z space are first converted to a PyTorch tensor and moved to the device (CPU or GPU):\n",
        "    codes = torch.from_numpy(codes).type(torch.FloatTensor).to(generator.run_device)\n",
        "    # Print information before mapping\n",
        "    print(f\"[INFO] Latent codes as tensor (before mapping to W space):\")\n",
        "    print(f\"Shape: {codes.shape}\")\n",
        "    print(f\"Sample data: {codes[0, :5]}\") #extracts the first 5 elements of the first latent vector.\n",
        "    #The codes are then passed through the StyleGAN's mapping network to convert them into the W space:\n",
        "    codes = generator.get_value(generator.model.mapping(codes))\n",
        "    # Print information after mapping to W space\n",
        "    print(f\"[INFO] Latent codes in W space (after mapping):\")\n",
        "    print(f\"Shape: {codes.shape}\")\n",
        "    print(f\"Sample data: {codes[0, :5]}\") #extracts the first 5 elements of the first latent vector.\n",
        "  #Finally, the function returns the latent codes, which can either be in the Z space or transformed to the W space, depending on the inputs.\n",
        "  #The mapping network transforms Z vectors into the W space, which is disentangled.\n",
        "  return codes\n",
        "\n",
        "\n",
        "#main function display a grid of images\n",
        "\n",
        "#images: A NumPy array containing multiple images. The expected shape is (num, height, width, channels), where:\n",
        "#col: The number of columns to display in the grid.\n",
        "#viz_size: The size (in pixels) to which each image will be resized before displaying (default is 256).\n",
        "def imshow(images, col, viz_size=256):\n",
        "  \"\"\"Shows images in one figure.\"\"\"\n",
        "  # Extracting the Shape of the Images\n",
        "  num, height, width, channels = images.shape\n",
        "  #Ensures that the number of images (num) is divisible by the number of columns (col).\n",
        "  #This makes sure that the images can be arranged into a complete grid without leaving any empty cells.\n",
        "  assert num % col == 0\n",
        "  #Computes the number of rows needed to fit all the images in the grid.\n",
        "  row = num // col\n",
        "\n",
        "  #Creates an empty canvas (a blank image) to hold all the images in a grid format.\n",
        "  #The canvas has dimensions (viz_size * row, viz_size * col, channels), where:\n",
        "  #viz_size * row: Total height of the grid.\n",
        "  #viz_size * col: Total width of the grid.\n",
        "  #channels: Number of color channels (same as the input images).\n",
        "  fused_image = np.zeros((viz_size * row, viz_size * col, channels), dtype=np.uint8)\n",
        "\n",
        "  #Placing Each Image on the Canvas\n",
        "  for idx, image in enumerate(images):\n",
        "    i, j = divmod(idx, col)\n",
        "    y = i * viz_size\n",
        "    x = j * viz_size\n",
        "    #Resizing and Copying Each Image to the Canvas\n",
        "    if height != viz_size or width != viz_size:\n",
        "      image = cv2.resize(image, (viz_size, viz_size))\n",
        "    fused_image[y:y + viz_size, x:x + viz_size] = image\n",
        "\n",
        "  #Converting the Canvas to a Displayable Format\n",
        "  fused_image = np.asarray(fused_image, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(fused_image).save(data, 'jpeg')\n",
        "  im_data = data.getvalue()\n",
        "  #Displaying the Image Using IPython\n",
        "  disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  return disp\n",
        "\n",
        "#This line is a special Jupyter Notebook cell annotation, display-mode: \"form\": Displays the code as a collapsible form in the notebook,\n",
        "#run: \"auto\": Automatically runs the cell whenever you change any of the parameters in the form.\n",
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "model_name = \"stylegan_ffhq\" #@param ['pggan_celebahq','stylegan_celebahq', 'stylegan_ffhq']\n",
        "latent_space_type = \"W\" #@param ['Z', 'W']\n",
        "\n",
        "#The function build_generator(model_name) is defined elsewhere and is responsible for loading the appropriate model based on the model_name parameter.\n",
        "generator = build_generator(model_name)\n",
        "\n",
        "# A list of attributes that you want to manipulate in the generated images.\n",
        "ATTRS = ['age', 'eyeglasses', 'gender', 'pose', 'smile']\n",
        "#boundaries: An empty dictionary that will store the attribute boundaries for each of the attributes in ATTRS\n",
        "boundaries = {}\n",
        "\n",
        "#Loading Attribute Boundaries\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  boundary_name = f'{model_name}_{attr_name}'\n",
        "  #np.load() loads the boundary vectors from .npy files, which contain pre-computed directions in the latent space that control the attributes.\n",
        "  if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "    boundaries[attr_name] = np.load(f'boundaries/{boundary_name}_w_boundary.npy')\n",
        "  else:\n",
        "    boundaries[attr_name] = np.load(f'boundaries/{boundary_name}_boundary.npy')\n",
        "\n",
        "    #@title { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "num_samples = 4 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "noise_seed = 0 #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "\n",
        "#sample_codes(): This function samples latent vectors based on the num_samples and noise_seed parameters.\n",
        "#The latent_space_type parameter specifies whether to sample in the Z space or the W space.\n",
        "#The result, latent_codes, is a batch of latent vectors that will be used to generate images.\n",
        "latent_codes = sample_codes(generator, num_samples, latent_space_type, noise_seed)\n",
        "# checks the type of generator being used.\n",
        "if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "  #If the generator is StyleGAN and the W space is selected, it sets synthesis_kwargs to include 'latent_space_type': 'W'.\n",
        "  synthesis_kwargs = {'latent_space_type': 'W'}\n",
        "else:\n",
        "  #For other cases (like Z space or PGGAN), no additional arguments are needed, so synthesis_kwargs remains an empty dictionary.\n",
        "  synthesis_kwargs = {}\n",
        "\n",
        "#synthesis_kwargs is a dictionary that contains optional arguments for the image synthesis function (easy_synthesize).\n",
        "#If the generator type is 'stylegan' and you have chosen to use the W space, then\n",
        "#synthesis_kwargs is set to {'latent_space_type': 'W'}.\n",
        "#This tells the easy_synthesize method to use the W space for generating images.\n",
        "\n",
        "#The **synthesis_kwargs unpacks the dictionary and passes its contents as keyword arguments({'latent_space_type': 'W'}) to the easy_synthesize() function.\n",
        "# 'latent_space_type': 'W' passed This ensures that the generator uses the W space for synthesis.\n",
        "# use synthesis_kwargs when You are using StyleGAN and want to generate images in the W space, which provides more control and disentangled features.\n",
        "# Exclude when You are using StyleGAN with the Z space.\n",
        "images = generator.easy_synthesize(latent_codes, **synthesis_kwargs)['image']\n",
        "imshow(images, col=num_samples)\n",
        "\n",
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "age = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "eyeglasses = 0 #@param {type:\"slider\", min:-2.9, max:3.0, step:0.1}\n",
        "gender = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "pose = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "smile = 0 #@param {type:\"slider\", min:-3.0, max:3.0, step:0.1}\n",
        "\n",
        "# Copying the Original Latent Codes\n",
        "new_codes = latent_codes.copy()\n",
        "\n",
        "#The loop iterates over the list of attributes (ATTRS), which is defined earlier in  code:\n",
        "# boundaries[attr_name]: pre-computed direction vector in the latent space\n",
        "#eval(attr_name): This converts the name of the attribute (e.g., 'age', 'smile') into its current value from the sliders.\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  new_codes += boundaries[attr_name] * eval(attr_name)\n",
        "\n",
        "new_images = generator.easy_synthesize(new_codes, **synthesis_kwargs)['image']\n",
        "imshow(new_images, col=num_samples)\n"
      ],
      "metadata": {
        "id": "hfppl_q3CNmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}