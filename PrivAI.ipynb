{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "PrivAI.ipynb",
      "authorship_tag": "ABX9TyMwO9JdsnlHmEIyDQcT+egr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamelof23/ASL/blob/main/PrivAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch Codebase and Models"
      ],
      "metadata": {
        "id": "iXIucaWWpoVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'interfacegan'\n",
        "!git clone https://github.com/genforce/interfacegan.git $CODE_DIR\n",
        "os.chdir(f'./{CODE_DIR}')\n",
        "!wget https://www.dropbox.com/s/t74z87pk3cf8ny7/pggan_celebahq.pth?dl=1 -O models/pretrain/pggan_celebahq.pth --quiet\n",
        "!wget https://www.dropbox.com/s/nmo2g3u0qt7x70m/stylegan_celebahq.pth?dl=1 -O models/pretrain/stylegan_celebahq.pth --quiet\n",
        "!wget https://www.dropbox.com/s/qyv37eaobnow7fu/stylegan_ffhq.pth?dl=1 -O models/pretrain/stylegan_ffhq.pth --quiet"
      ],
      "metadata": {
        "id": "tyMo0VCfprkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Utility Functions"
      ],
      "metadata": {
        "id": "XpgN0pa0pwg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "import io\n",
        "import IPython.display\n",
        "import numpy as np\n",
        "import cv2\n",
        "import PIL.Image\n",
        "\n",
        "import torch\n",
        "\n",
        "# A dictionary containing information about the models available (e.g., pggan, stylegan).\n",
        "from models.model_settings import MODEL_POOL\n",
        "\n",
        "from models.pggan_generator import PGGANGenerator\n",
        "from models.stylegan_generator import StyleGANGenerator\n",
        "from utils.manipulator import linear_interpolate\n",
        "\n",
        "\n",
        "def build_generator(model_name):\n",
        "  \"\"\"Builds the generator by model name.\"\"\"\n",
        "  gan_type = MODEL_POOL[model_name]['gan_type']\n",
        "  if gan_type == 'pggan':\n",
        "    generator = PGGANGenerator(model_name)\n",
        "  elif gan_type == 'stylegan':\n",
        "    generator = StyleGANGenerator(model_name)\n",
        "  return generator\n",
        "\n",
        "\n",
        "def sample_codes(generator, num, latent_space_type='Z', seed=0):\n",
        "  \"\"\"Samples latent codes randomly.\"\"\"\n",
        "  #Setting a random seed ensures reproducibility. Every time the function is called with the same seed, it generates the same latent codes are typically sampled from a standard normal distribution.\n",
        "  np.random.seed(seed)\n",
        "  #The function uses generator.easy_sample(num) to sample num latent vectors from the Z space.\n",
        "  codes = generator.easy_sample(num)\n",
        "  print(f\"[INFO] Latent codes in Z space (before mapping):\")\n",
        "  print(f\"Shape: {codes.shape}\")\n",
        "  print(f\"Sample data: {codes[0, :5]}\")  # Print first 5 elements of the first vector\n",
        "  #This condition checks if the generator is using StyleGAN and whether we want to use the W space.\n",
        "  if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "    #he codes sampled in Z space are first converted to a PyTorch tensor and moved to the device (CPU or GPU):\n",
        "    codes = torch.from_numpy(codes).type(torch.FloatTensor).to(generator.run_device)\n",
        "    # Print information before mapping\n",
        "    print(f\"[INFO] Latent codes as tensor (before mapping to W space):\")\n",
        "    print(f\"Shape: {codes.shape}\")\n",
        "    print(f\"Sample data: {codes[0, :5]}\") #extracts the first 5 elements of the first latent vector.\n",
        "    #The codes are then passed through the StyleGAN's mapping network to convert them into the W space:\n",
        "    codes = generator.get_value(generator.model.mapping(codes))\n",
        "    # Print information after mapping to W space\n",
        "    print(f\"[INFO] Latent codes in W space (after mapping):\")\n",
        "    print(f\"Shape: {codes.shape}\")\n",
        "    print(f\"Sample data: {codes[0, :5]}\") #extracts the first 5 elements of the first latent vector.\n",
        "  #Finally, the function returns the latent codes, which can either be in the Z space or transformed to the W space, depending on the inputs.\n",
        "  #The mapping network transforms Z vectors into the W space, which is disentangled.\n",
        "  return codes\n",
        "\n",
        "\n",
        "#main function display a grid of images\n",
        "\n",
        "#images: A NumPy array containing multiple images. The expected shape is (num, height, width, channels), where:\n",
        "#col: The number of columns to display in the grid.\n",
        "#viz_size: The size (in pixels) to which each image will be resized before displaying (default is 256).\n",
        "def imshow(images, col, viz_size=256):\n",
        "  \"\"\"Shows images in one figure.\"\"\"\n",
        "  # Extracting the Shape of the Images\n",
        "  num, height, width, channels = images.shape\n",
        "  #Ensures that the number of images (num) is divisible by the number of columns (col).\n",
        "  #This makes sure that the images can be arranged into a complete grid without leaving any empty cells.\n",
        "  assert num % col == 0\n",
        "  #Computes the number of rows needed to fit all the images in the grid.\n",
        "  row = num // col\n",
        "\n",
        "  #Creates an empty canvas (a blank image) to hold all the images in a grid format.\n",
        "  #The canvas has dimensions (viz_size * row, viz_size * col, channels), where:\n",
        "  #viz_size * row: Total height of the grid.\n",
        "  #viz_size * col: Total width of the grid.\n",
        "  #channels: Number of color channels (same as the input images).\n",
        "  fused_image = np.zeros((viz_size * row, viz_size * col, channels), dtype=np.uint8)\n",
        "\n",
        "  #Placing Each Image on the Canvas\n",
        "  for idx, image in enumerate(images):\n",
        "    i, j = divmod(idx, col)\n",
        "    y = i * viz_size\n",
        "    x = j * viz_size\n",
        "    #Resizing and Copying Each Image to the Canvas\n",
        "    if height != viz_size or width != viz_size:\n",
        "      image = cv2.resize(image, (viz_size, viz_size))\n",
        "    fused_image[y:y + viz_size, x:x + viz_size] = image\n",
        "\n",
        "  #Converting the Canvas to a Displayable Format\n",
        "  fused_image = np.asarray(fused_image, dtype=np.uint8)\n",
        "  data = io.BytesIO()\n",
        "  PIL.Image.fromarray(fused_image).save(data, 'jpeg')\n",
        "  im_data = data.getvalue()\n",
        "  #Displaying the Image Using IPython\n",
        "  disp = IPython.display.display(IPython.display.Image(im_data))\n",
        "  return disp"
      ],
      "metadata": {
        "id": "-s7RBNULp0Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select a Model"
      ],
      "metadata": {
        "id": "SE18UQjbqHuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This line is a special Jupyter Notebook cell annotation, display-mode: \"form\": Displays the code as a collapsible form in the notebook,\n",
        "#run: \"auto\": Automatically runs the cell whenever you change any of the parameters in the form.\n",
        "#@title { display-mode: \"form\", run: \"auto\" }\n",
        "model_name = \"stylegan_ffhq\" #@param ['pggan_celebahq','stylegan_celebahq', 'stylegan_ffhq']\n",
        "latent_space_type = \"W\" #@param ['Z', 'W']\n",
        "\n",
        "#The function build_generator(model_name) is defined elsewhere and is responsible for loading the appropriate model based on the model_name parameter.\n",
        "generator = build_generator(model_name)\n",
        "\n",
        "# A list of attributes that you want to manipulate in the generated images.\n",
        "ATTRS = ['age', 'eyeglasses', 'gender', 'pose', 'smile']\n",
        "#boundaries: An empty dictionary that will store the attribute boundaries for each of the attributes in ATTRS\n",
        "boundaries = {}\n",
        "\n",
        "#Loading Attribute Boundaries\n",
        "for i, attr_name in enumerate(ATTRS):\n",
        "  boundary_name = f'{model_name}_{attr_name}'\n",
        "  #np.load() loads the boundary vectors from .npy files, which contain pre-computed directions in the latent space that control the attributes.\n",
        "  if generator.gan_type == 'stylegan' and latent_space_type == 'W':\n",
        "    boundaries[attr_name] = np.load(f'boundaries/{boundary_name}_w_boundary.npy')\n",
        "  else:\n",
        "    boundaries[attr_name] = np.load(f'boundaries/{boundary_name}_boundary.npy')"
      ],
      "metadata": {
        "id": "8jeFZnt3qBqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload latent codes"
      ],
      "metadata": {
        "id": "eVYhSVzVqS5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load your custom latent vector (1, 512) from a .npy file\n",
        "latent_codes = np.load('/content/sample_data/your_latent_vector.npy')\n",
        "\n",
        "# Check the shape to ensure it matches (1, 512)\n",
        "print(f\"[INFO] Loaded latent vector shape: {latent_codes.shape}\")\n",
        "\n",
        "# Ensure that the latent vector is in the correct format (numpy.ndarray) and shape\n",
        "if not isinstance(latent_codes, np.ndarray) or latent_codes.shape != (1, 512):\n",
        "    raise ValueError(f\"Latent codes must be a numpy.ndarray with shape (1, 512), but got {latent_codes.shape}\")\n",
        "\n",
        "# Use W space for StyleGAN synthesis\n",
        "synthesis_kwargs = {'latent_space_type': 'W'}\n",
        "\n",
        "# Generate the image using your custom latent vector\n",
        "images = generator.easy_synthesize(latent_codes, **synthesis_kwargs)['image']\n",
        "\n",
        "# Display the generated images using your imshow function\n",
        "imshow(images, col=1)\n"
      ],
      "metadata": {
        "id": "L1t9sQI7qXFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Noise"
      ],
      "metadata": {
        "id": "q538xAYyqe5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Differential Privacy Parameters (Slider Values)\n",
        "epsilon = 5  # @param {\"type\":\"slider\",\"min\":0.01,\"max\":5,\"step\":0.01}\n",
        "delta = 0.001  # @param {type:\"slider\", min:1e-7, max:1e-3, step:1e-7}\n",
        "clipping_threshold = 0.1  # @param {type:\"slider\", min:0.1, max:5.0, step:0.1}\n",
        "\n",
        "# Dropdown Menu for Attribute Selection\n",
        "attribute_selection = \"Age\"  # @param [\"Age\", \"Gender\", \"Both\"]\n",
        "\n",
        "# Sliders for the Selected Attributes\n",
        "if attribute_selection in [\"Age\", \"Both\"]:\n",
        "    age = 1  # @param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
        "else:\n",
        "    age = 0  # Disable age if not selected\n",
        "\n",
        "if attribute_selection in [\"Gender\", \"Both\"]:\n",
        "    gender = 1  # @param {type:\"slider\", min:0.0, max:3.0, step:0.1}\n",
        "else:\n",
        "    gender = 0  # Disable gender if not selected\n",
        "\n",
        "# Function to add Gaussian noise for differential privacy\n",
        "def add_gaussian_noise(epsilon, delta, boundary):\n",
        "    \"\"\"\n",
        "    Adds Gaussian noise to a vector for differential privacy.\n",
        "\n",
        "    Args:\n",
        "        epsilon (float): Privacy budget.\n",
        "        delta (float): Probability of failure.\n",
        "        boundary (np.ndarray): Attribute boundary vector.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Noise scaled by boundary.\n",
        "    \"\"\"\n",
        "    # Compute the standard deviation for Gaussian noise\n",
        "    sigma = np.sqrt(2 * np.log(1.25 / delta)) / epsilon\n",
        "\n",
        "    # Generate Gaussian noise\n",
        "    noise = np.random.normal(0, sigma, boundary.shape)\n",
        "\n",
        "    # Return the noise scaled by the boundary\n",
        "    return boundary * noise\n",
        "\n",
        "# Copy the original latent codes\n",
        "new_codes = latent_codes.copy()\n",
        "\n",
        "# Add noise only to the selected attributes\n",
        "for attr_name in [\"age\", \"gender\"]:  # Only iterate over selected attributes\n",
        "    attr_value = eval(attr_name)  # Get the slider value for the current attribute\n",
        "\n",
        "    # Skip attributes with a slider value of 0\n",
        "    if attr_value == 0:\n",
        "        continue\n",
        "\n",
        "    # Add Gaussian noise to the boundary\n",
        "    noised_boundary = add_gaussian_noise(\n",
        "        epsilon=epsilon,\n",
        "        delta=delta,\n",
        "        boundary=boundaries[attr_name]\n",
        "    )\n",
        "\n",
        "    # Add the noise-modified boundary to the latent codes\n",
        "    new_codes += noised_boundary * attr_value\n",
        "\n",
        "# Apply clipping as the final step\n",
        "new_codes = np.clip(new_codes, -clipping_threshold, clipping_threshold)\n",
        "\n",
        "# Generate and display the new images\n",
        "new_images = generator.easy_synthesize(new_codes, **synthesis_kwargs)['image']\n",
        "imshow(new_images, col=num_samples)\n"
      ],
      "metadata": {
        "id": "Ic-vCQ6qqj0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}